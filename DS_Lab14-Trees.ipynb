{
 "metadata": {
  "name": "",
  "signature": "sha256:52b4170c47d99511b08467d599c023e384b4cd29e237dde57d89ac0cd0fe3042"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<style>\n",
      "@font-face {\n",
      "  font-family: CharisSILW;\n",
      "  src: url(files/CharisSIL-R.woff);\n",
      "}\n",
      "@font-face {\n",
      "  font-family: CharisSILW;\n",
      "  font-style: italic;\n",
      "  src: url(files/CharisSIL-I.woff);\n",
      "}\n",
      "@font-face {\n",
      "\tfont-family: CharisSILW;\n",
      "\tfont-weight: bold;\n",
      "\tsrc: url(files/CharisSIL-B.woff);\n",
      "}\n",
      "@font-face {\n",
      "\tfont-family: CharisSILW;\n",
      "\tfont-weight: bold;\n",
      "\tfont-style: italic;\n",
      "\tsrc: url(files/CharisSIL-BI.woff);\n",
      "}\n",
      "\n",
      "div.cell, div.text_cell_render{\n",
      "    max-width:1000px;\n",
      "}\n",
      "\n",
      "h1 {\n",
      "    text-align:center;\n",
      "    font-family: Charis SIL, CharisSILW, serif;\n",
      "}\n",
      "\n",
      ".rendered_html {\n",
      "    font-size: 130%;\n",
      "    line-height: 1.3;\n",
      "}\n",
      "\n",
      ".rendered_html li {\n",
      "    line-height: 2;\n",
      "}\n",
      "\n",
      ".rendered_html h1{\n",
      "    line-height: 1.3;\n",
      "}\n",
      "\n",
      ".rendered_html h2{\n",
      "    line-height: 1.2;\n",
      "}\n",
      "\n",
      ".rendered_html h3{\n",
      "    line-height: 1.0;\n",
      "}\n",
      "\n",
      ".text_cell_render {\n",
      "    font-family: Charis SIL, CharisSILW, serif;\n",
      "    line-height: 145%;\n",
      "}\n",
      "\n",
      "li li {\n",
      "    font-size: 85%;\n",
      "}\n",
      "</style>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab\n",
      "==========================================\n",
      "Gradient Boosted Regression Trees\n",
      "------------------------------------------\n",
      "Alessandro D. Gagliardi  \n",
      "*(adapted from Peter Prettenhofer's tutorial)*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Scikit-learn provides two estimators for gradient boosting: ``GradientBoostingClassifier`` and ``GradientBoostingRegressor``, both are located in the ``sklearn.ensemble`` package:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import GradientBoostingClassifier\n",
      "from sklearn.ensemble import GradientBoostingRegressor"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Estimators support arguments to control the fitting behaviour -- these arguments are often called _hyperparameters_. Among the most important ones for GBRT are:\n",
      "\n",
      "  * number of regression trees (``n_estimators``)\n",
      "  * depth of each individual tree (``max_depth``)\n",
      "  * loss function (``loss``)\n",
      "\n",
      "For example if you want to fit a regression model with 100 trees of depth 3 using least-squares:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est = GradientBoostingRegressor(n_estimators=100, max_depth=3, loss='ls')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is an self-contained example that shows how to fit a ``GradientBoostingClassifier`` to a synthetic dataset:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import make_hastie_10_2\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "# generate synthetic data from ESLII - Example 10.2\n",
      "X, y = make_hastie_10_2(n_samples=5000)\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
      "\n",
      "# fit estimator\n",
      "est = GradientBoostingClassifier(n_estimators=200, max_depth=3)\n",
      "est.fit(X_train, y_train)\n",
      "\n",
      "# predict class labels\n",
      "pred = est.predict(X_test)\n",
      "\n",
      "# score on test data (accuracy)\n",
      "acc = est.score(X_test, y_test)\n",
      "print('ACC: %.4f' % acc)\n",
      "\n",
      "# predict class probabilities\n",
      "est.predict_proba(X_test)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The state of the estimator is stored in instance attributes that have a trailing underscore ('\\_'). For example, the sequence of regression trees (``DecisionTreeRegressor`` objects) is stored in ``est.estimators_``:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "est.estimators_[0, 0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Gradient Boosted Regression Trees in Practise\n",
      "\n",
      "### Function approximation\n",
      "\n",
      "  * Sinoide function + random gaussian noise \n",
      "  * 80 training (blue), 20 test (red) points"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import numpy as np\n",
      "from sklearn.cross_validation import train_test_split\n",
      "\n",
      "FIGSIZE = (11, 7)\n",
      "\n",
      "def ground_truth(x):\n",
      "    \"\"\"Ground truth -- function to approximate\"\"\"\n",
      "    return x * np.sin(x) + np.sin(2 * x)\n",
      "\n",
      "def gen_data(n_samples=200):\n",
      "    \"\"\"generate training and testing data\"\"\"\n",
      "    np.random.seed(15)\n",
      "    X = np.random.uniform(0, 10, size=n_samples)[:, np.newaxis]\n",
      "    y = ground_truth(X.ravel()) + np.random.normal(scale=2, size=n_samples)\n",
      "    train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
      "    return X_train, X_test, y_train, y_test\n",
      "\n",
      "X_train, X_test, y_train, y_test = gen_data(100)\n",
      "\n",
      "# plot ground truth\n",
      "x_plot = np.linspace(0, 10, 500)\n",
      "\n",
      "def plot_data(alpha=0.4, s=20):\n",
      "    fig = plt.figure(figsize=FIGSIZE)\n",
      "    gt = plt.plot(x_plot, ground_truth(x_plot), alpha=alpha, label='ground truth')\n",
      "\n",
      "    # plot training and testing data\n",
      "    plt.scatter(X_train, y_train, s=s, alpha=alpha)\n",
      "    plt.scatter(X_test, y_test, s=s, alpha=alpha, color='red')\n",
      "    plt.xlim((0, 10))\n",
      "    plt.ylabel('y')\n",
      "    plt.xlabel('x')\n",
      "    \n",
      "annotation_kw = {'xycoords': 'data', 'textcoords': 'data',\n",
      "                 'arrowprops': {'arrowstyle': '->', 'connectionstyle': 'arc'}}\n",
      "    \n",
      "plot_data()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Regression Trees\n",
      "\n",
      "  * ``max_depth`` argument controlls the depth of the tree\n",
      "  * The deeper the tree the more variance can be explained"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.tree import DecisionTreeRegressor\n",
      "plot_data()\n",
      "est = DecisionTreeRegressor(max_depth=1).fit(X_train, y_train)\n",
      "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
      "         label='RT max_depth=1', color='g', alpha=0.9, linewidth=2)\n",
      "\n",
      "est = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n",
      "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
      "         label='RT max_depth=3', color='g', alpha=0.7, linewidth=1)\n",
      "\n",
      "plt.legend(loc='upper left')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Function approximation with Gradient Boosting\n",
      "\n",
      "  * ``n_estimators`` argument controls the number of trees\n",
      "  * ``staged_predict`` method allows us to step through predictions as we add more trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import islice\n",
      "\n",
      "plot_data()\n",
      "est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0)\n",
      "est.fit(X_train, y_train)\n",
      "\n",
      "ax = plt.gca()\n",
      "first = True\n",
      "\n",
      "# step through prediction as we add 10 more trees.\n",
      "for pred in islice(est.staged_predict(x_plot[:, np.newaxis]), 0, est.n_estimators, 10):\n",
      "    plt.plot(x_plot, pred, color='r', alpha=0.2)\n",
      "    if first:\n",
      "        ax.annotate('High bias - low variance', xy=(x_plot[x_plot.shape[0] // 2],\n",
      "                                                    pred[x_plot.shape[0] // 2]),\n",
      "                                                    xytext=(4, 4), **annotation_kw)\n",
      "        first = False\n",
      "\n",
      "pred = est.predict(x_plot[:, np.newaxis])\n",
      "plt.plot(x_plot, pred, color='r', label='GBRT max_depth=1')\n",
      "ax.annotate('Low bias - high variance', xy=(x_plot[x_plot.shape[0] // 2],\n",
      "                                            pred[x_plot.shape[0] // 2]),\n",
      "                                            xytext=(6.25, -6), **annotation_kw)\n",
      "plt.legend(loc='upper left')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model complexity\n",
      "\n",
      "  * The number of trees and the depth of the individual trees control model complexity\n",
      "  * Model complexity comes at a price: **overfitting**\n",
      "  \n",
      "  \n",
      "### Deviance plot\n",
      "\n",
      "  * Diagnostic to determine if model is overfitting\n",
      "  * Plots the training/testing error (deviance) as a function of the number of trees (=model complexity)\n",
      "  * Training error (deviance) is stored in ``est.train_score_``\n",
      "  * Test error is computed using ``est.staged_predict``"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def deviance_plot(est, X_test, y_test, ax=None, label='', train_color='#2c7bb6', \n",
      "                  test_color='#d7191c', alpha=1.0, ylim=(0, 10)):\n",
      "    \"\"\"Deviance plot for ``est``, use ``X_test`` and ``y_test`` for test error. \"\"\"\n",
      "    n_estimators = len(est.estimators_)\n",
      "    test_dev = np.empty(n_estimators)\n",
      "\n",
      "    for i, pred in enumerate(est.staged_predict(X_test)):\n",
      "       test_dev[i] = est.loss_(y_test, pred)\n",
      "\n",
      "    if ax is None:\n",
      "        fig = plt.figure(figsize=FIGSIZE)\n",
      "        ax = plt.gca()\n",
      "        \n",
      "    ax.plot(np.arange(n_estimators) + 1, test_dev, color=test_color, label='Test %s' % label, \n",
      "             linewidth=2, alpha=alpha)\n",
      "    ax.plot(np.arange(n_estimators) + 1, est.train_score_, color=train_color, \n",
      "             label='Train %s' % label, linewidth=2, alpha=alpha)\n",
      "    ax.set_ylabel('Error')\n",
      "    ax.set_xlabel('n_estimators')\n",
      "    ax.set_ylim(ylim)\n",
      "    return test_dev, ax\n",
      "\n",
      "test_dev, ax = deviance_plot(est, X_test, y_test)\n",
      "ax.legend(loc='upper right')\n",
      "\n",
      "# add some annotations\n",
      "ax.annotate('Lowest test error', xy=(test_dev.argmin() + 1, test_dev.min() + 0.02),\n",
      "            xytext=(150, 3.5), **annotation_kw)\n",
      "\n",
      "ann = ax.annotate('', xy=(800, test_dev[799]),  xycoords='data',\n",
      "                  xytext=(800, est.train_score_[799]), textcoords='data',\n",
      "                  arrowprops={'arrowstyle': '<->'})\n",
      "ax.text(810, 3.5, 'train-test gap')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Overfitting\n",
      "\n",
      "  * Model has too much capacity and starts fitting the idiosyncracies of the training data\n",
      "  * Indicated by a large gap between train and test error\n",
      "  * GBRT provides a number of knobs to control overfitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Regularization\n",
      "\n",
      "  * Tree structure\n",
      "  * Shrinkage\n",
      "  * Stochastic Gradient Boosting\n",
      "\n",
      "## Tree Structure\n",
      "\n",
      "  * The ``max_depth`` of the trees controls the degree of features interactions (variance++)\n",
      "  * Use ``min_samples_leaf`` to have a sufficient number of samples per leaf (bias++)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fmt_params(params):\n",
      "    return \", \".join(\"{0}={1}\".format(key, val) for key, val in params.iteritems())\n",
      "\n",
      "fig = plt.figure(figsize=FIGSIZE)\n",
      "ax = plt.gca()\n",
      "for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n",
      "                                          ({'min_samples_leaf': 3}, ('#fdae61', '#abd9e9'))]:\n",
      "    est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, \n",
      "                                    learning_rate=1.0)\n",
      "    est.set_params(**params)\n",
      "    est.fit(X_train, y_train)\n",
      "    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
      "                                 train_color=train_color, test_color=test_color)\n",
      "    \n",
      "ax.annotate('Higher bias', xy=(900, est.train_score_[899]), xytext=(600, 3), **annotation_kw)\n",
      "ax.annotate('Lower variance', xy=(900, test_dev[899]), xytext=(600, 3.5), **annotation_kw)\n",
      "plt.legend(loc='upper right')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Shrinkage\n",
      "\n",
      " * Slow learning by shrinking the predictions of each tree by some small scalar (``learning_rate``)\n",
      " * A lower ``learning_rate`` requires a higher number of ``n_estimators``\n",
      " * Its a trade-off between runtime against accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=FIGSIZE)\n",
      "ax = plt.gca()\n",
      "for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n",
      "                                          ({'learning_rate': 0.1},\n",
      "                                           ('#fdae61', '#abd9e9'))]:\n",
      "    est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0)\n",
      "    est.set_params(**params)\n",
      "    est.fit(X_train, y_train)\n",
      "    \n",
      "    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
      "                                 train_color=train_color, test_color=test_color)\n",
      "    \n",
      "ax.annotate('Requires more trees', xy=(200, est.train_score_[199]), \n",
      "            xytext=(300, 1.75), **annotation_kw)\n",
      "ax.annotate('Lower test error', xy=(900, test_dev[899]),\n",
      "            xytext=(600, 1.75), **annotation_kw)\n",
      "plt.legend(loc='upper right')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stochastic Gradient Boosting\n",
      "\n",
      " * Subsampling the training set before growing each tree (``subsample``)\n",
      " * Subsampling the features before finding the best split node (``max_features``)\n",
      " * Latter usually works better if there is a sufficient large number of features"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fig = plt.figure(figsize=FIGSIZE)\n",
      "ax = plt.gca()\n",
      "for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n",
      "                                          ({'learning_rate': 0.1, 'subsample': 0.5},\n",
      "                                           ('#fdae61', '#abd9e9'))]:\n",
      "    est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0,\n",
      "                                    random_state=1)\n",
      "    est.set_params(**params)\n",
      "    est.fit(X_train, y_train)\n",
      "    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
      "                                 train_color=train_color, test_color=test_color)\n",
      "    \n",
      "ax.annotate('Even lower test error', xy=(400, test_dev[399]),\n",
      "            xytext=(500, 3.0), **annotation_kw)\n",
      "\n",
      "est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0,\n",
      "                                subsample=0.5)\n",
      "est.fit(X_train, y_train)\n",
      "test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params({'subsample': 0.5}),\n",
      "                             train_color='#abd9e9', test_color='#fdae61', alpha=0.5)\n",
      "ax.annotate('Subsample alone does poorly', xy=(300, test_dev[299]), \n",
      "            xytext=(500, 5.5), **annotation_kw)\n",
      "plt.legend(loc='upper right', fontsize='small')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Hyperparameter tuning\n",
      "\n",
      "I usually follow this recipe to tune the hyperparameters:\n",
      "\n",
      "  1. Pick ``n_estimators`` as large as (computationally) possible (e.g. 3000)\n",
      "  2. Tune ``max_depth``, ``learning_rate``, ``min_samples_leaf``, and ``max_features`` via grid search\n",
      "  3. Increase ``n_estimators`` even more and tune ``learning_rate`` again holding the other parameters fixed"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "param_grid = {'learning_rate': [0.1, 0.01, 0.001],\n",
      "              'max_depth': [4, 6],\n",
      "              'min_samples_leaf': [3, 5]  ## depends on the nr of training examples\n",
      "              # 'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n",
      "              }\n",
      "\n",
      "est = GradientBoostingRegressor(n_estimators=3000)\n",
      "# this may take some minutes\n",
      "gs_cv = GridSearchCV(est, param_grid, scoring='mean_squared_error', n_jobs=4).fit(X_train, y_train)\n",
      "\n",
      "# best hyperparameter setting\n",
      "print('Best hyperparameters: %r' % gs_cv.best_params_)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# refit model on best parameters\n",
      "est.set_params(**gs_cv.best_params_)\n",
      "est.fit(X_train, y_train)\n",
      "\n",
      "# plot the approximation\n",
      "plot_data()\n",
      "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='r', linewidth=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Caution: Hyperparameters interact with each other (``learning_rate`` and ``n_estimators``, ``learning_rate`` and ``subsample``, ``max_depth`` and ``max_features``).\n",
      "\n",
      "See [G. Ridgeway, \"Generalized boosted models: A guide to the gbm package\", 2005](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=540A0A638283F64E251B1342248CDBCF?doi=10.1.1.151.4024&rep=rep1&type=pdf)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1-2 Pairs\n",
      "==========================================\n",
      "California Housing\n",
      "------------------------------------------\n",
      "<img src=\"assets/cal-housing.png\" />  \n",
      "\n",
      " * Predict the median house value for census block groups in California\n",
      " * 20.000 groups, 8 features: *median income*, *average house age*, *latitude*, *longitude*, ...\n",
      " * Mean Absolute Error on 80-20 train-test split"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets.california_housing import fetch_california_housing\n",
      "\n",
      "cal_housing = fetch_california_housing()\n",
      "\n",
      "# split 80/20 train-test\n",
      "X_train, X_test, y_train, y_test = train_test_split(cal_housing.data,\n",
      "                                                    cal_housing.target,\n",
      "                                                    test_size=0.2,\n",
      "                                                    random_state=1)\n",
      "names = cal_housing.feature_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Challenges\n",
      "\n",
      "  * heterogenous features (different scales and distributions, see plot below)\n",
      "  * non-linear feature interactions (interaction: latitude and longitude)\n",
      "  * extreme responses (robust regression techniques)\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import mean_absolute_error\n",
      "from sklearn.dummy import DummyRegressor\n",
      "from sklearn.tree import DecisionTreeRegressor\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.ensemble.partial_dependence import plot_partial_dependence"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Task\n",
      "----\n",
      "1. Plot histograms of the `Latitude`, `Longitude`, Median Income (`MedInc`), and Median House Value (`y_train`)\n",
      "2. Evaluate and compare the `mean_absolute_error` of \n",
      "    - `GradientBoostingRegressor` \n",
      "    - `RandomForestRegressor`\n",
      "    - `DecisionTreeRegressor`\n",
      "    - `DummyRegressor`\n",
      "3. Tune the models (except `DummyRegressor`) as shown above  \n",
      "    (this time include `max_features` as a hyperparameter):\n",
      "    - Use the `deviance_plot` function defined above to check whether you are in the high-bias or high-variance regime.\n",
      "    - Hint: for `DecisionTreeRegressor` and `RandomForestRegressor`, \n",
      "        look at the docs to see which hyperparameters need tuning.  \n",
      "4. Plot feature importance (using the `.feature_importances_` property)\n",
      "    - Hint: load `feature_importances_` into a `pandas.Series` and use `.plot(kind='barh')` \n",
      "5. `plot_partial_dependence` on `MedInc`, `AveOccup`, `HouseAge`, and `('AveOccup', 'HouseAge')`.\n",
      "    - `('AveOccup', 'HouseAge')` will show you the interaction between these two features."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Homework\n",
      "========\n",
      "Go through the following tutorials on your own:\n",
      "\n",
      "1. Decision Trees: http://scikit-learn.org/stable/modules/tree.html\n",
      "2. Random Forests: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_iris.html\n",
      "\n",
      "Then:\n",
      "\n",
      "- Finish the California Housing assignment if you were not able to finish it in class."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
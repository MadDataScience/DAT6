{
 "metadata": {
  "name": "",
  "signature": "sha256:3278c863cb09b0369aa68c78415bffd2313d55ed167f365006b02b3228581096"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab\n",
      "==========================================\n",
      "Hadoop Streaming\n",
      "------------------------------------------\n",
      "Alessandro D. Gagliardi  \n",
      "*(adapted from [Michael G. Noll's tutorial](http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/))*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Go to [console.aws.amazon.com/s3](https://console.aws.amazon.com/s3/home?region=us-east-1) and create a bucket for this course. Bucket names must be universally unique.  \n",
      "Try using the name of your github repo, but put it all in lower case as EMR may not like upper case in S3 buckets."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "$ pip install awscli\n",
      "$ aws configure\n",
      "```\n",
      "Use the same AWS Access Key ID and AWS Secret Access Key you used when configuring StarCluster."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Replace `<my_bucket>` with the name of your bucket:\n",
      "```bash\n",
      "$ aws s3 ls s3://<my_bucket>\n",
      "```\n",
      "Should return nothing (if there is nothing in there). If it returns an error, raise your hand!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, copy: \n",
      "\n",
      "* The Outline of Science, Vol. 1 (of 4) by J. Arthur Thomson (pg20417.txt)\n",
      "* The Notebooks of Leonardo Da Vinci (pg5000.txt)\n",
      "* Ulysses, by James Joyce (pg4300.txt)\n",
      "\n",
      "to your S3 bucket thus:\n",
      "```bash\n",
      "$ aws s3 cp pg20417.txt s3://<my_bucket>/gutenberg/\n",
      "$ aws s3 cp pg5000.txt s3://<my_bucket>/gutenberg/\n",
      "$ aws s3 cp pg4300.txt s3://<my_bucket>/gutenberg/\n",
      "```\n",
      "Make sure your files are there:\n",
      "\n",
      "```bash\n",
      "$ aws s3 ls s3://<my_bucket>/gutenberg/\n",
      "```\n",
      "should return something like:\n",
      "\n",
      "    2014-06-07 19:35:23     674570 pg20417.txt\n",
      "    2014-06-07 19:34:53    1573150 pg4300.txt\n",
      "    2014-06-07 19:34:35    1423803 pg5000.txt\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Follow the instructions at https://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/CLI_CreateStreaming.html to create an EMR cluster.  \n",
      "\n",
      "* Be sure to select an EC2 key pair.\n",
      "* Don't worry about adding a streaming step as we will be doing that manually."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* For your core nodes, try picking 5 r3.4xlarge instances and request spot instances at 15\u00a2. The master node can remain small."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file mapper.py\n",
      "#!/usr/bin/env python\n",
      "\n",
      "import sys\n",
      "\n",
      "def read_input(file):\n",
      "    for line in file:\n",
      "        # split the line into words\n",
      "        yield line.split()\n",
      "\n",
      "def main(separator='\\t'):\n",
      "    # input comes from STDIN (standard input)\n",
      "    data = read_input(sys.stdin)\n",
      "    for words in data:\n",
      "        # write the results to STDOUT (standard output);\n",
      "        # what we output here will be the input for the\n",
      "        # Reduce step, i.e. the input for reducer.py\n",
      "        #\n",
      "        # tab-delimited; the trivial word count is 1\n",
      "        for word in words:\n",
      "            print '%s%s%d' % (word, separator, 1)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%file reducer.py\n",
      "#!/usr/bin/env python\n",
      "\n",
      "from itertools import groupby\n",
      "from operator import itemgetter\n",
      "import sys\n",
      "\n",
      "def read_mapper_output(file, separator='\\t'):\n",
      "    for line in file:\n",
      "        yield line.rstrip().split(separator, 1)\n",
      "\n",
      "def main(separator='\\t'):\n",
      "    # input comes from STDIN (standard input)\n",
      "    data = read_mapper_output(sys.stdin, separator=separator)\n",
      "    # groupby groups multiple word-count pairs by word,\n",
      "    # and creates an iterator that returns consecutive keys and their group:\n",
      "    #   current_word - string containing a word (the key)\n",
      "    #   group - iterator yielding all [\"&lt;current_word&gt;\", \"&lt;count&gt;\"] items\n",
      "    for current_word, group in groupby(data, itemgetter(0)):\n",
      "        try:\n",
      "            total_count = sum(int(count) for current_word, count in group)\n",
      "            print \"%s%s%d\" % (current_word, separator, total_count)\n",
      "        except ValueError:\n",
      "            # count was not a number, so silently discard this item\n",
      "            pass\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to make these files executable:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "chmod a+x mapper.py\n",
      "chmod a+x reducer.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's test the mapper phase:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "echo \"foo foo quux labs foo bar quux\" | ./mapper.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The shuffle phase works like `sort` in bash:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "echo \"foo foo quux labs foo bar quux\" | ./mapper.py | sort"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The whole pipeline:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "echo \"foo foo quux labs foo bar quux\" | ./mapper.py | sort | ./reducer.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try it on one of our files:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat pg20417.txt | ./mapper.py | head "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "cat pg20417.txt | ./mapper.py | sort | ./reducer.py | head "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that python's `.split()` method is not a very good tokenizer as it fails to remove punctuation. NLTK has guidelines on how to create better tokenizers."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you are satisfied, upload mapper and reducer to s3:\n",
      "```bash\n",
      "$ aws s3 cp mapper.py s3://<my_bucket>/\n",
      "$ aws s3 cp reducer.py s3://<my_bucket>/\n",
      "```\n",
      "Make sure everything is there:\n",
      "```bash\n",
      "$ aws s3 ls s3://<my_bucket>\n",
      "```\n",
      "should return something like:\n",
      "\n",
      "                               PRE gutenberg/\n",
      "    2014-06-07 19:25:57        540 mapper.py\n",
      "    2014-06-07 19:26:03       1034 reducer.py"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "SSH into hadoop cluster once up and running:\n",
      "```bash\n",
      "$ ssh hadoop@ec2-XXX-XXX-XXX-XXX.us-west-2.compute.amazonaws.com -i ~/.ssh/<my_public_key>.pem\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Verify that Hadoop can see your files on S3:\n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop fs -ls s3n://<my_bucket>/gutenberg/\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, run your Hadoop streaming job:\n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop jar contrib/streaming/hadoop-streaming.jar \\\n",
      "    -input s3n://<my_bucket>/gutenberg/* \\\n",
      "    -output s3n://<my_bucket>/gutenberg-output \\\n",
      "    -mapper s3n://<my_bucket>/mapper.py map \\\n",
      "    -reducer s3n://<my_bucket>/reducer.py reduce \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how much longer this takes than doing it locally. Hadoop streaming is extreamly inefficient because it has to read and write to disk at every step of the way. EMR is especially inefficient because the disk, in each case, may be miles away from the CPU. That said, it does make jobs possible that would otherwise be impossible."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check the output:\n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop fs -ls s3n://<my_bucket>/gutenberg-output\n",
      "```\n",
      "        Found 53 items\n",
      "        -rwxrwxrwx   1          0 2014-06-08 02:54 /gutenberg-output/_SUCCESS\n",
      "        -rwxrwxrwx   1      16601 2014-06-08 02:53 /gutenberg-output/part-00000\n",
      "        -rwxrwxrwx   1      17278 2014-06-08 02:53 /gutenberg-output/part-00001\n",
      "        -rwxrwxrwx   1      17231 2014-06-08 02:53 /gutenberg-output/part-00002\n",
      "        -rwxrwxrwx   1      17419 2014-06-08 02:53 /gutenberg-output/part-00003\n",
      "        -rwxrwxrwx   1      16259 2014-06-08 02:53 /gutenberg-output/part-00004\n",
      "        -rwxrwxrwx   1      17051 2014-06-08 02:53 /gutenberg-output/part-00005\n",
      "                                        .\n",
      "                                        .\n",
      "                                        .\n",
      "        -rwxrwxrwx   1      16660 2014-06-08 02:54 /gutenberg-output/part-00049\n",
      "        -rwxrwxrwx   1      17177 2014-06-08 02:54 /gutenberg-output/part-00050\n",
      "        -rwxrwxrwx   1      16510 2014-06-08 02:54 /gutenberg-output/part-00051\n",
      "    \n",
      "```bash\n",
      "hadoop@ip-XX-XX-XX-XX:~$ hadoop fs -cat s3n://<my_bucket>/gutenberg-output/part-00000 | head\n",
      "```\n",
      "        \"A\t2\n",
      "        \"Alpine-glow\"\t1\n",
      "        \"Ignoramus\":\t1\n",
      "        \"It\t3\n",
      "        \"O\"\t1\n",
      "        \"Year\"\t2\n",
      "        \"Your\t2\n",
      "        \"_in_\t1\n",
      "        \"grouse\t2\n",
      "        \"planetoids,\"\t1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "1-2 Pairs\n",
      "==========================================\n",
      "Wikipedia XML Data\n",
      "------------------------------------------\n",
      "\n",
      "* Use the same Wikipedia data we used on Monday at `s3n://datasets.elasticmapreduce/wikipediaxml/`.\n",
      "* Convert the script we used into mapper and reducer scripts similar to the ones above.\n",
      "* Run it!\n",
      "* Don't forget to terminate your cluster before you leave class!!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Homework\n",
      "==========================================\n",
      "Projects!\n",
      "------------------------------------------\n",
      "Next week is devoted to working on your projects. The EiRs and I will be here to assist you. Come with questions!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
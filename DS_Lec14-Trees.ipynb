{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:33c03228926c29bea03bdb6d3b16f8ab0a5d8c75fe90062677ce2ad7931c5afa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Science\n",
      "================================\n",
      "Trees & Forests\n",
      "--------------------------------\n",
      "Alessandro D. Gagliardi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Agenda:\n",
      "--------------------------------\n",
      "\n",
      "1. Decision Trees\n",
      "    1. Building Decision Trees\n",
      "    2. Optimization Functions\n",
      "    3. Preventing Overfit\n",
      "2. Ensemble Methods\n",
      "    1. Random Forests\n",
      "    2. Gradient Boosted Trees"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is a decision tree?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A non-parametric hierachical classification technique"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**non-parametric**: no parameters, no distribution assumptions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**hierarchical**: consists of a sequence of questions which yield a class label when applied to any record"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "How is a decision tree represented?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Using a configuration of **nodes** and **edges**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "More concretely, as a _multiway tree_, which is a type of (directed acyclic) **graph**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "In a decision tree, the nodes represent questions (**test conditions**) and the edges are the answers to these questions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What are the types of nodes?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The top node of the tree is called the **root node**. This node has 0 incoming edges, and 2+ outgoing edges."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "An **internal node** has 1 incoming edge, and 2+ outgoing edges. Internal nodes represent test conditions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A **leaf node** has 1 incoming edge and, 0 outgoing edges. Leaf nodes correspond to class labels."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "The nodes in our tree are connected by _directed_ edges.  \n",
      "These directed edges lead from _parent nodes_ to _child nodes_."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"assets/example_data_decision_trees1.png\" width=\"800\" />  \n",
      "<small>source: http://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"assets/example_data_decision_trees2.png\" width=\"600\" />  \n",
      "<small>source: http://www-users.cs.umn.edu/~kumar/dmbook/ch4.pdf</small>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Review: Decision Trees\n",
      "--------------------------------------------------------\n",
      "Classify data using a sequence of questions, assuming nothing about the data\n",
      "\n",
      "Represented by nodes and splits\n",
      "\n",
      "Interpreted with a top to bottom approach, using nodes to representing a root, test conditions, and class labels."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Building Decision Trees"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "How do we build a decision tree?\n",
      "--------------------------------------------------------\n",
      "One possibility would be to evaluate all possible decision trees (eg, all permutations of test conditions) for a given dataset."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "But this is generally too complex to be practical $\\rightarrow O(2^n)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "How do we find a practical solution that works?\n",
      "--------------------------------------------------------\n",
      "We use a **heuristic**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "That is, a fast solution good enough to solve the problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The basic method used to build (or \u201cgrow\u201d) a decision tree is **Hunt\u2019s algorithm**.\n",
      "--------------------------------------------------------\n",
      "This is a greedy recursive algorithm that leads to a local optimum."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**greedy** \u2013 algorithm makes locally optimal decision at each step"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**recursive** \u2013 splits task into subtasks, solves each the same way`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**local optimum** \u2013 solution for a given neighborhood of points"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Hunt\u2019s algorithm builds a decision tree by recursively partitioning records into smaller & smaller subsets."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The partitioning decision is made at each node according to a metric called **purity**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A partition is 100% pure when *all of its records belong to a single class*."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Consider a binary classification problem with classes $X, Y$. Given a set of records $D_t$ at node $t$, Hunt\u2019s algorithm proceeds as follows:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "1)  If all records in $D_t$ belong to class $X$, then $t$ is a leaf node corresponding to class $X$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "2)  If $D_t$ contains records from both classes, then a test condition is created to partition the records further. In this case, $t$ is an internal node whose outgoing edges correspond to the possible outcomes of this test condition."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "These outgoing edges terminate in **child nodes**. A record $d$ in $D_t$ is assigned to one of these child nodes based on the outcome of the test condition applied to $d$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "3)  These steps are then recursively applied to each child node."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Decision trees are easy to interpret, but the algorithms to create them are a bit complicated."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "How do we partition the training records?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Test conditions can create **binary splits**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Alternatively, we can create **multiway splits**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Multiway splits can produce purer subsets, but may lead to overfitting!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For continuous features, we can use either method"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "How do we determine the best split?\n",
      "--------------------------------------------------------\n",
      "Recall that no split is necessary (at a given node) when all records belong to the same class."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Therefore we want each step to create the partition with the highest possible purity."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We need an objective function to optimize!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Review\n",
      "--------------------------------------------------------\n",
      "Decision trees are generated with a greedy algorithm that determines to find the best tree\n",
      "\n",
      "Splits are generated based on either binary or multi splits of either discrete or continuous data\n",
      "\n",
      "A decision tree is complete when all data belongs to a single partition"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Optimization Functions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We want our objective function to measure the gain in purity from a particular split."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Therefore we want it to depend on the *class distribution* over the nodes (before and after the split)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For example, let $p(i|t)$ be the probability of class $i$ at node $t$ (eg, the fraction of records labeled $i$ at node $t$)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We are using the frequentist definition of probability here!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Then for a binary $(0/1)$ classification problem,"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The *minimum purity partition* is given by the distribution:  \n",
      "$$ p(0|t) = p(1|t) = 0.5 $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The *maximum purity partition* is given (eg) by the distribution:\n",
      "$$ p(0|t) = 1 \u2013 p(1|t) = 1 $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Some measures of impurity include:  \n",
      "$$ Entropy(t) = -\\sum_{i=0}^{c-1}p(i|t) log_2 p(i|t) \n",
      "\\\\ Gini(t) = 1 - \\sum_{i=0}^{c-1}[p(i|t)]^2\n",
      "\\\\ Classification error(t) = 1 - \\max_i[p(i|t)] $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"assets/dt_impurity2.png\" width=\"600\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Despite consistency, different measures may create different splits."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Generally speaking, a test condition with a high number of outcomes can lead to overfitting (ex: a split with one outcome per record)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "One way of dealing with this is to restrict the algorithm to binary splits only (CART)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Another way is to use a splitting criterion which explicitly penalizes the number of outcomes (C4.5)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can use a function of the information gain called the gain ratio to explicitly penalize high numbers of outcomes:  \n",
      "\n",
      "$$ gain\\: ratio = \\frac{\\Delta_{info}}{-\\sum p(v_i) log_2 p(v_i)} $$\n",
      "\n",
      "(Where $p(v_i)$ refers to the probability of label $i$ at node $v$)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This is a form of regularization!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Impurity measures put us on the right track, but on their own they are not enough to tell us how our split will do."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Why is this true?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We still need to look at impurity before & after the split."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can make this comparison using the **gain**:  \n",
      "$$ \\Delta = I(parent) - \\sum_{children} \\frac{N_j}{N}I(child_j) $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "(Here $I$ is the impurity measure, $N_j$ denotes the number of records at child node $j$, and $N$ denotes the number of records at the parent node.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "N.B. When $I$ is the entropy, this quantity is called the **information gain**."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Prevent Overfitting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "In addition to determining splits, we also need a stopping criterion to tell us when we\u2019re done."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "For example, we can stop when all records belong to the same class, or when all records have the same attributes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This is correct in principle, but would likely lead to overfitting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "One possibility is **pre-pruning**, which involves setting a minimum threshold on the gain, and stopping when no split achieves a gain above this threshold."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This prevents overfitting, but is difficult to calibrate in practice (may preserve bias!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Alternatively we could build the full tree, and then perform **pruning** as a post-processing step."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "To prune a tree, we examine the nodes from the bottom-up and simplify pieces of the tree (according to some criteria)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Complicated subtrees can be replaced either with a single node, or with a simpler (child) subtree."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The first approach is called **subtree replacement**, and the second is **subtree raising**."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Review\n",
      "--------------------------------------------------------\n",
      "A decision tree is complete when all results can belong to a single partition\n",
      "\n",
      "To prevent overfitting, we prune trees after compleition\n",
      "\n",
      "Pruning is a form of generalization that simplifies a decision tree"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "*In practice, decision trees are seldem used alone*\n",
      "-----------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "*Instead we use ensembles to improve our model*\n",
      "-----------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Intro to ensemble methods\n",
      "========================="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Goals\n",
      "\n",
      "- What are ensemble methods, and why should we use them?\n",
      "- What kind of machine learning problems can they solve?\n",
      "- What are some kinds of ensemble methods?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What are ensemble methods?\n",
      "--------------------------------------------------------------------------\n",
      "*Ensemble methods use multiple variant models and aggregate over them to determine a better prediction*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Goal: Provide better answer to prediction than using only base"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Base classifiers**: \"Weak\" Learners"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Ensemble classifiers**: \"Strong\" Learners"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can't just assume all ensemble learning methods will always outperform!\n",
      "--------------------------------------------------------------------------\n",
      "Base classifier must be more accurate than random guessing\n",
      "\n",
      "Base classifier's error must occur through different training sets (cross validation)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What kind of machine learning problems can ensemble learning solve?\n",
      "--------------------------------------------------------------------------\n",
      "Just about *anything**"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Regressions: prediction of a continuous value"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Classification: prediction of a class label"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Clustering (kind of): create new labels from data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What does this look like?\n",
      "--------------------------------------------------------------------------\n",
      "![Ensemble](assets/ensembles_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What are different kinds of ensemble learning techniques?\n",
      "--------------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Bagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "* Boosting"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "For today, we'll look at what each of these mean, and how we can apply them to decision trees\n",
      "--------------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Review\n",
      "--------------------------------------------------------------------------\n",
      "Ensemble methods are ways to use multiple variances of a model to create a better prediction\n",
      "\n",
      "Ensemble methods can be used to solve a variety of machine learning problems\n",
      "\n",
      "Three common techniques are bagging, forests, and boosting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Bagging\n",
      "=============="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Goals:\n",
      "\n",
      "* What is bagging?\n",
      "* What are the advantages/disadvantages of bagging?\n",
      "* What's an example of how bagging improves a base classifier?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is bagging?\n",
      "--------------------------------------------------------------------------\n",
      "Bagging is an ensemble method short for bootstrap aggregating"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Useful for when you don't have a lot of data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Each model is built independently of each other"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Explain that again\n",
      "--------------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Bootstrapping:** Duplicating data $k$ at random to produce a larger sample set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**Aggregating:** Devising a mean/average of a final output created by multiple models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Question: Does this remind anyone of something we learned about already?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This should remind students about k-fold cross validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "One more time...\n",
      "--------------------------------------------------------------------------\n",
      "Take $k$ different samples of training set to produce $k$ base classifiers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Each sample set is the same size of original training set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We get it to the original size by duplicating the sample"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We then average $k$ classifiers to get a final prediction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What are the advantages of bagging?\n",
      "--------------------------------------------------------------------------\n",
      "Bagging reduces **variance error**\n",
      "\n",
      "Leftover error is typically narrowed down to **bias error**\n",
      "\n",
      "Generally less likely to overfit\n",
      "\n",
      "In terms of machine learning problems, bagged clustering is very reasonable (\"k-means bagging\")"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Review variance! Ask someone what variance is (refer back to the dartboard example)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What are the disadvantages of bagging?\n",
      "--------------------------------------------------------------------------\n",
      "High amount of error in bias (generally)\n",
      "\n",
      "For trees, simple structure no longer exists"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Example\n",
      "--------------------------------------------------------------------------\n",
      "![Bagging](assets/ensembles_1.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Bagging: A Review\n",
      "--------------------------------------------------------------------------\n",
      "Bagging is an ensemble method that builds independent classifiers based on a training set\n",
      "\n",
      "A final classifier is built through an average of the base classifiers built\n",
      "\n",
      "Bagging can be used on a variety of machine learning problems"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Random Forests\n",
      "=============="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Goals\n",
      "\n",
      "* What are random forests?\n",
      "* What makes random forests different from bagging?\n",
      "* What do random forests get used for?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What are random forests?\n",
      "--------------------------------------------------------------------------\n",
      "Random Forests are a form of bagging that use a random variable to create classifiers (hence, \"random\" forests)\n",
      "\n",
      "Algorithm developed at Bell Labs\n",
      "\n",
      "Sometimes referred to \"fancy bagging\""
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What's so fancy about random forests?\n",
      "--------------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Random forests take the concept of bagging and produces $k$ trees based on a random sampling of features in the data set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Each tree is based on a few features (instead of all features) of data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "All the trees together create a forest"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "[Example](https://bigml.com/user/BigML/gallery/model/4f8991e1035d0737d700002d)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Random Forests: Advantages\n",
      "--------------------------------------------------------------------------\n",
      "RFs try to decorrelate trees to reduce autocorrelation."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Basically, goal is to make each tree as independent as possible"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Remember, this is the core behind bagging"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "RFs don't require cross validation. Instead we monitor \"out of bag\" error rate, and cross validation happens internally"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "This means we don't need to prune trees (usually)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "OOB: Times a class is not equal to the true class averaged over all cases"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Generally, a cleaner version of bagging with little tradeoff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Random Forests: Disadvantages (the trade offs)\n",
      "--------------------------------------------------------------------------\n",
      "Can have high bias when categorical variables have a lot of values"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Determining random value features has to be considered based on number of correlating features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Random Forests: Review\n",
      "--------------------------------------------------------------------------\n",
      "Random forests use a random sample of features to build k trees\n",
      "\n",
      "Useful for solving any problem similar enough to bagging (regression and classification, specifically)\n",
      "\n",
      "Generally considered an improvement over bagging with little trade off"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Boosting\n",
      "=============="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "### Goals\n",
      "\n",
      "* What is boosting?\n",
      "* What makes boosting so different from bagging and forests?\n",
      "* How does boosting perform when compared to bagging and forests?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "What is boosting?\n",
      "--------------------------------------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Like bagging:  \n",
      "An aggregation method that averages base model variants"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Unlike bagging:  \n",
      "Uses each iteration of previous model to provide weighted classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Each iteration has a higher weight until we reach the final answer to the problem"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "You could say that each previous iteration \"boosts\" the performance of the next iteration"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Example\n",
      "--------------------------------------------------------------------------\n",
      "![Boosting](assets/boosting.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "![Residual Fitting](assets/residual_fitting_2.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What are the advantages to using this weighted ensemble method?\n",
      "--------------------------------------------------------------------------\n",
      "Unlike bagging, which improves variance, _boosting improves bias_\n",
      "\n",
      "When you have a clean data set, boosting can perform much faster than bagging, and come to a better answer sooner"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "What are the disadvantages to boosting?\n",
      "--------------------------------------------------------------------------\n",
      "Boosting is prone to overfitting!\n",
      "\n",
      "Particularly true if boosting follows noise instead of a clean signal\n",
      "\n",
      "Cannot be easily parallelized. (In bagging or random forests, each tree can be trained in parallel. In boosting, each tree depends upon the output of the previous one.)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Hence, the importance of having a good, clean data set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Boosting: Review\n",
      "--------------------------------------------------------------------------\n",
      "Boosting is an iterative, aggregative ensemble method\n",
      "\n",
      "Boosting provides weights to each iteration of k\n",
      "\n",
      "Though it has some limitations, boosting could be the best bang for your buck!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Questions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "LAB\n",
      "===\n",
      "In the `DAT6` folder, from the command line:\n",
      "```bash\n",
      "git commit -am ...\n",
      "git checkout gh-pages\n",
      "git pull\n",
      "git checkout personal\n",
      "git merge gh-pages\n",
      "ipython notebook\n",
      "```\n",
      "Then open `DS_Lab14-Trees`"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
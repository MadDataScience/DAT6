{
 "metadata": {
  "name": "",
  "signature": "sha256:824d172841f1d1ad7df99d0b3bd4c5ef1602691b5217bffdad5e5c3213ebf429"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab\n",
      "==========================================\n",
      "Text Feature Extraction for Classification\n",
      "------------------------------------------\n",
      "Alessandro D. Gagliardi  \n",
      "*(adapted from Olivier Grisel's tutorial)*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "_What is scikit-learn?_\n",
      "--------------------------------------------------------\n",
      "\n",
      "* Library of Machine Learning algorithms\n",
      "* Focus on standard methods (e.g. ESL-II, (Elements of Statistical Learning))\n",
      "* Open Source (BSD)\n",
      "* Simple &nbsp; ** ` fit / predict / transform ` ** &nbsp; API\n",
      "* Python / NumPy / SciPy / Cython\n",
      "* Model Assessment, Selection & Ensembles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"assets/predictive_modeling_data_flow.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<style>\n",
      "div.input {\n",
      "    width: 105ex; /* about 80 chars + buffer */\n",
      "}\n",
      "div.text_cell {\n",
      "    width: 105ex; /* instead of 100%, */\n",
      "}\n",
      "div.text_cell_render {\n",
      "    /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/\n",
      "    font-family: \"Charis SIL\", serif !important; /* Make non-code text serif. */\n",
      "    line-height: 145% !important; /* added for some line spacing of text. */\n",
      "    width: 105ex !important; /* instead of 'inherit' for shorter lines */\n",
      "}\n",
      "/* Set the size of the headers */\n",
      "div.text_cell_render h1 {\n",
      "    font-size: 18pt;\n",
      "}\n",
      "div.text_cell_render h2 {\n",
      "    font-size: 14pt;\n",
      "}\n",
      ".CodeMirror {\n",
      "     font-family: Consolas, monospace;\n",
      "}\n",
      "</style>\n",
      "\n",
      "Outline of this section:\n",
      "\n",
      "- Turn a corpus of text documents into **feature vectors** using a **Bag of Words** representation,\n",
      "- Train a simple text classifier on the feature vectors,\n",
      "- Wrap the vectorizer and the classifier with a **pipeline**,\n",
      "- Cross-validation and **model selection** on the pipeline."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "# Some nice default configuration for plots\n",
      "plt.rcParams['figure.figsize'] = 10, 7.5\n",
      "plt.rcParams['axes.grid'] = True\n",
      "plt.gray()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Check that you have the datasets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%run fetch_data.py"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls -lh datasets/"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Text Classification in 20 lines of Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's start by implementing a canonical text classification example:\n",
      "\n",
      "- The 20 newsgroups dataset: around 18000 text posts from 20 newsgroups forums\n",
      "- Bag of Words features extraction with TF-IDF weighting\n",
      "- Naive Bayes classifier or Linear Support Vector Machine for the classifier itself"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_files\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "# Load the text data\n",
      "categories = [\n",
      "    'alt.atheism',\n",
      "    'talk.religion.misc',\n",
      "    'comp.graphics',\n",
      "    'sci.space',\n",
      "]\n",
      "twenty_train_small = load_files('datasets/20news-bydate-train/',\n",
      "    categories=categories, encoding='latin-1')\n",
      "twenty_test_small = load_files('datasets/20news-bydate-test/',\n",
      "    categories=categories, encoding='latin-1')\n",
      "\n",
      "# Turn the text documents into vectors of word frequencies\n",
      "vectorizer = TfidfVectorizer(min_df=2)\n",
      "X_train = vectorizer.fit_transform(twenty_train_small.data)\n",
      "y_train = twenty_train_small.target\n",
      "\n",
      "# Fit a classifier on the training set\n",
      "classifier = MultinomialNB().fit(X_train, y_train)\n",
      "print(\"Training score: {0:.1f}%\".format(\n",
      "    classifier.score(X_train, y_train) * 100))\n",
      "\n",
      "# Evaluate the classifier on the testing set\n",
      "X_test = vectorizer.transform(twenty_test_small.data)\n",
      "y_test = twenty_test_small.target\n",
      "print(\"Testing score: {0:.1f}%\".format(\n",
      "    classifier.score(X_test, y_test) * 100))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "[Multinomial Naive Bayes](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)\n",
      "--------------------------------------------------------\n",
      "`MultinomialNB` implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors $\\theta_y = (\\theta_{y1},\\ldots,\\theta_{yn})$ for each class $y$, where $n$ is the number of features (in text classification, the size of the vocabulary) and $\\theta_{yi}$ is the probability $P(x_i \\mid y)$ of feature $i$ appearing in a sample belonging to class $y$.\n",
      "\n",
      "The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
      "\n",
      "$$ \\hat{\\theta}_{yi} = \\frac{ N_{yi} + \\alpha}{N_y + \\alpha n} $$\n",
      "\n",
      "where $N_{yi} = \\sum_{x \\in T} x_i$ is the number of times feature $i$ appears in a sample of class $y$ in the training set $T$, and $N_{y} = \\sum_{i=1}^{|T|} N_{yi}$ is the total count of all features for class $y$.\n",
      "\n",
      "The smoothing priors $\\alpha \\ge 0$ accounts for features not present in the learning samples and prevents zero probabilities in further computations. Setting $\\alpha = 1$ is called Laplace smoothing, while $\\alpha < 1$ is called Lidstone smoothing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's now decompose what we just did to understand and customize each step."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Loading the Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's explore the dataset loading utility without passing a list of categories: in this case we load the full 20 newsgroups dataset in memory. The source website for the 20 newsgroups already provides a date-based train / test split that is made available using the `subset` keyword argument: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls datasets/"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls -lh datasets/20news-bydate-train"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ls -lh datasets/20news-bydate-train/alt.atheism/ | head -n27"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The `load_files` function can load text files from a 2 levels folder structure assuming folder names represent categories:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(load_files.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_train = load_files('datasets/20news-bydate-train/',\n",
      "  encoding='latin-1', random_state=42)\n",
      "all_twenty_test = load_files('datasets/20news-bydate-test/',\n",
      "    encoding='latin-1', random_state=42)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_target_names = all_twenty_train.target_names\n",
      "all_target_names"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_train.target"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_train.target.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_twenty_test.target.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(all_twenty_train.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(all_twenty_train.data[0])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def display_sample(i, dataset):\n",
      "    print(\"Class name: \" + dataset.target_names[dataset.target[i]])\n",
      "    print(\"Text content:\\n\")\n",
      "    print(dataset.data[i])"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display_sample(0, all_twenty_train)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "display_sample(1, all_twenty_train)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Let's compute the (uncompressed, in-memory) size of the training and test sets in MB assuming an 8-bit encoding (in this case, all chars can be encoded using the latin-1 charset)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def text_size(text, charset='iso-8859-1'):\n",
      "    return len(text.encode(charset)) * 8 * 1e-6\n",
      "\n",
      "train_size_mb = sum(text_size(text) for text in all_twenty_train.data) \n",
      "test_size_mb = sum(text_size(text) for text in all_twenty_test.data)\n",
      "\n",
      "print(\"Training set size: {0} MB\".format(int(train_size_mb)))\n",
      "print(\"Testing set size: {0} MB\".format(int(test_size_mb)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "If we only consider a small subset of the 4 categories selected from the initial example:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_small_size_mb = sum(text_size(text) for text in twenty_train_small.data) \n",
      "test_small_size_mb = sum(text_size(text) for text in twenty_test_small.data)\n",
      "\n",
      "print(\"Training set size: {0} MB\".format(int(train_small_size_mb)))\n",
      "print(\"Testing set size: {0} MB\".format(int(test_small_size_mb)))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Extracting Text Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Terms that occur in only a few documents are often more valuable than ones that occur in many \u2013 inverse document frequency (${IDF}_j$)\n",
      "* The more often a term occurs in a document, the more likely it is to be important for that document \u2013 term frequency (${TF}_{ij}$)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "TfidfVectorizer()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(min_df=1)\n",
      "\n",
      "%time X_train_small = vectorizer.fit_transform(twenty_train_small.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The results is not a `numpy.array` but instead a `scipy.sparse` matrix. _(Similar to the DocumentTermMatrix in R's `tm` library.)_ This datastructure is quite similar to a 2D numpy array but it does not store the zeros."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_small"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "scipy.sparse matrices also have a shape attribute to access the dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples, n_features = X_train_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This dataset has around 2000 samples (the rows of the data matrix):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This is the same value as the number of strings in the original list of text documents:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(twenty_train_small.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The columns represent the individual token occurrences:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_features"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "This number is the size of the vocabulary of the model extracted during fit in a Python dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vectorizer.vocabulary_)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The keys of the `vocabulary_` attribute are also called feature names and can be accessed as a list of strings."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(vectorizer.get_feature_names())"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Here are the first 10 elements (sorted in lexicographical order):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()[:10]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Let's have a look at the features from the middle:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.get_feature_names()[n_features / 2:n_features / 2 + 10]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Training a Classifier on Text Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We have previously extracted a vector representation of the training corpus and put it into a variable name `X_train_small`. To train a supervised model, in this case a classifier, we also need "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_small = twenty_train_small.target"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_train_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "We can shape that we have the same number of samples for the input data and the labels:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_small.shape[0] == y_train_small.shape[0]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can now train a classifier, for instance a Multinomial Naive Bayesian classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.naive_bayes import MultinomialNB\n",
      "\n",
      "clf = MultinomialNB(alpha=0.1)\n",
      "clf"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(X_train_small, y_train_small)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "We can now evaluate the classifier on the testing set. Let's first use the builtin score function, which is the rate of correct classification in the test set:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test_small = vectorizer.transform(twenty_test_small.data)\n",
      "y_test_small = twenty_test_small.target"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_test_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_test_small.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_test_small, y_test_small)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "We can also compute the score on the train set and observe that the model is both overfitting and underfitting a bit at the same time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.score(X_train_small, y_train_small)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Alternative evaluation metrics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Na\u00efve Bayes is a probabilistic models: instead of just predicting a binary outcome (alt.atheism or talk.religion) given the input features it can also estimates the posterior probability of the outcome given the input features using the `predict_proba` method:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_predicted_proba = clf.predict_proba(X_test_small)\n",
      "target_predicted_proba[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By default the decision threshold is 0.5: if we vary the decision threshold from 0 to 1 we could generate a family of binary classifier models that address all the possible trade offs between false positive and false negative prediction errors.\n",
      "\n",
      "We can summarize the performance of a binary classifier for all the possible thresholds by plotting the ROC curves and quantifying the area under the curve (AUC):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_roc_curve(target_test, target_predicted_proba, categories):\n",
      "    from sklearn.metrics import roc_curve\n",
      "    from sklearn.metrics import auc\n",
      "    \n",
      "    for pos_label, category in enumerate(categories):\n",
      "        fpr, tpr, thresholds = roc_curve(target_test, target_predicted_proba[:, pos_label], pos_label)\n",
      "        roc_auc = auc(fpr, tpr)\n",
      "        plt.plot(fpr, tpr, label='{} ROC curve (area = {:.3f})'.format(category, roc_auc))\n",
      "    \n",
      "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
      "    plt.xlim([0.0, 1.0])\n",
      "    plt.ylim([0.0, 1.0])\n",
      "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
      "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
      "    plt.title('Receiver Operating Characteristic')\n",
      "    plt.legend(loc=\"lower right\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_roc_curve(y_test_small, target_predicted_proba, twenty_test_small.target_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here the area under ROC curve ranges between .963 and .974. The ROC-AUC score of a random model is expected to 0.5 on average while the accuracy score of a random model depends on the class imbalance of the data. ROC-AUC can be seen as a way to callibrate the predictive accuracy of a model against class imbalance."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Introspecting the Behavior of the Text Vectorizer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The text vectorizer has many parameters to customize it's behavior, in particular how it extracts tokens:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TfidfVectorizer()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(TfidfVectorizer.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The easiest way to introspect what the vectorizer is actually doing for a given test of parameters is call the `vectorizer.build_analyzer()` to get an instance of the text analyzer it uses to process the text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer = TfidfVectorizer().build_analyzer()\n",
      "analyzer(\"I love scikit-learn: this is a cool Python lib!\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "You can notice that all the tokens are lowercase, that the single letter word \"I\" was dropped, and that hyphenation is used. Let's change some of that default behavior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "analyzer = TfidfVectorizer(\n",
      "    preprocessor=lambda text: text,  # disable lowercasing\n",
      "    token_pattern=ur'(?u)\\b[\\w-]+\\b', # treat hyphen as a letter\n",
      "                                      # do not exclude single letter tokens\n",
      ").build_analyzer()\n",
      "\n",
      "analyzer(\"I love scikit-learn: this is a cool Python lib!\")"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The analyzer name comes from the Lucene parlance: it wraps the sequential application of:\n",
      "\n",
      "- text preprocessing (processing the text documents as a whole, e.g. lowercasing)\n",
      "- text tokenization (splitting the document into a sequence of tokens)\n",
      "- token filtering and recombination (e.g. n-grams extraction, see later)\n",
      "\n",
      "The analyzer system of scikit-learn is much more basic than lucene's though."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Exercise**:\n",
      "\n",
      "- Write a pre-processor callable (e.g. a python function) to remove the headers of the text a newsgroup post.\n",
      "- Vectorize the data again and measure the impact on performance of removing the header info from the dataset.\n",
      "- Do you expect the performance of the model to improve or decrease? What is the score of a uniform random classifier on the same dataset?\n",
      "\n",
      "Hint: the `TfidfVectorizer` class can accept python functions to customize the `preprocessor`, `tokenizer` or `analyzer` stages of the vectorizer.\n",
      "    \n",
      "- type `TfidfVectorizer()` alone in a cell to see the default value of the parameters\n",
      "\n",
      "- type `TfidfVectorizer.__doc__` to print the constructor parameters doc or `?` suffix operator on a any Python class or method to read the docstring or even the `??` operator to read the source code."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Solution**:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's write a Python function to strip the post headers and only retain the body (text after the first blank line):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def strip_headers(post):\n",
      "    \"\"\"Find the first blank line and drop the headers to keep the body\"\"\"\n",
      "    if '\\n\\n' in post:\n",
      "        headers, body = post.split('\\n\\n', 1)\n",
      "        return body.lower()\n",
      "    else:\n",
      "        # Unexpected post inner-structure, be conservative\n",
      "        # and keep everything\n",
      "        return post.lower()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's try it on the first post. Here is the original post content, including the headers:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "original_text = all_twenty_train.data[0]\n",
      "print(original_text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is the result of applying our header stripping function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text_body = strip_headers(original_text)\n",
      "print(text_body)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's plug our function in the vectorizer and retrain a naive Bayes classifier (as done initially):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "strip_vectorizer = TfidfVectorizer(preprocessor=strip_headers, min_df=2)\n",
      "X_train_small_stripped = strip_vectorizer.fit_transform(\n",
      "    twenty_train_small.data)\n",
      "\n",
      "y_train_small_stripped = twenty_train_small.target\n",
      "\n",
      "classifier = MultinomialNB().fit(\n",
      "  X_train_small_stripped, y_train_small_stripped)\n",
      "\n",
      "print(\"Training score: {0:.1f}%\".format(\n",
      "    classifier.score(X_train_small_stripped, y_train_small_stripped) * 100))\n",
      "\n",
      "X_test_small_stripped = strip_vectorizer.transform(twenty_test_small.data)\n",
      "y_test_small_stripped = twenty_test_small.target\n",
      "print(\"Testing score: {0:.1f}%\".format(\n",
      "    classifier.score(X_test_small_stripped, y_test_small_stripped) * 100))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So indeed the header data is making the problem easier (cheating one could say) but naive Bayes classifier can still guess 80% of the time against 1 / 4 == 25% mean score for a random guessing on the small subset with 4 target categories."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Model Selection of the Naive Bayes Classifier Parameter Alone"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The `MultinomialNB` class is a good baseline classifier for text as it's fast and has few parameters to tweak:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MultinomialNB()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(MultinomialNB.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "By reading the doc we can see that the `alpha` parameter is a good candidate to adjust the model for the bias (underfitting) vs variance (overfitting) trade-off."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Setting Up a Pipeline for Cross Validation and Model Selection of the Feature Extraction parameters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "The feature extraction class has many options to customize its behavior:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(TfidfVectorizer.__doc__)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "In order to evaluate the impact of the parameters of the feature extraction one can chain a configured feature extraction and classifier:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.pipeline import Pipeline\n",
      "\n",
      "pipeline = Pipeline((\n",
      "    ('vec', TfidfVectorizer()),\n",
      "    ('clf', MultinomialNB()),\n",
      "))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Such a pipeline can then be cross validated or even grid searched:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from scipy.stats import sem\n",
      "\n",
      "scores = cross_val_score(pipeline, twenty_train_small.data,\n",
      "                         twenty_train_small.target, cv=3, n_jobs=3)\n",
      "scores.mean(), sem(scores)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "For the grid search, the parameters names are prefixed with the name of the pipeline step using \"__\" as a separator:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.grid_search import GridSearchCV\n",
      "\n",
      "parameters = {\n",
      "    'vec__max_df': [0.8, 1.0],\n",
      "    'vec__ngram_range': [(1, 1), (1, 2)],\n",
      "    'clf__alpha': np.logspace(-5, 0, 6)\n",
      "}\n",
      "\n",
      "gs = GridSearchCV(pipeline, parameters, verbose=2, refit=False, n_jobs=3)\n",
      "_ = gs.fit(twenty_train_small.data, twenty_train_small.target)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs.best_score_"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gs.best_params_"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Introspecting Model Performance"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Displaying the Most Discriminative Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "Let's fit a model on the small dataset and collect info on the fitted components:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pipeline = Pipeline((\n",
      "    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True)),\n",
      "    ('clf', MultinomialNB(alpha = 0.001)),\n",
      "))\n",
      "_ = pipeline.fit(twenty_train_small.data, twenty_train_small.target)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec_name, vec = pipeline.steps[0]\n",
      "clf_name, clf = pipeline.steps[1]\n",
      "\n",
      "feature_names = vec.get_feature_names()\n",
      "target_names = twenty_train_small.target_names\n",
      "\n",
      "feature_weights = clf.coef_\n",
      "\n",
      "feature_weights.shape"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "By sorting the feature weights on the linear model and asking the vectorizer what their names is, one can get a clue on what the model did actually learn on the data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def display_important_features(feature_names, target_names, weights, n_top=30):\n",
      "    for i, target_name in enumerate(target_names):\n",
      "        print(u\"Class: \" + target_name)\n",
      "        print(u\"\")\n",
      "        \n",
      "        sorted_features_indices = weights[i].argsort()[::-1]\n",
      "        \n",
      "        most_important = sorted_features_indices[:n_top]\n",
      "        print(u\", \".join(u\"{0}: {1:.4f}\".format(feature_names[j], weights[i, j])\n",
      "                        for j in most_important))\n",
      "        print(u\"...\")\n",
      "        \n",
      "        least_important = sorted_features_indices[-n_top:]\n",
      "        print(u\", \".join(u\"{0}: {1:.4f}\".format(feature_names[j], weights[i, j])\n",
      "                        for j in least_important))\n",
      "        print(u\"\")\n",
      "        \n",
      "display_important_features(feature_names, target_names, feature_weights)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Displaying the per-class Classification Reports"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report\n",
      "\n",
      "predicted = pipeline.predict(twenty_test_small.data)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(classification_report(twenty_test_small.target, predicted,\n",
      "                            target_names=twenty_test_small.target_names))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "Printing the Confusion Matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "source": [
      "The confusion matrix summarize which class where by having a look at off-diagonal entries: here we can see that articles about atheism have been wrongly classified as being about religion 57 times for instance: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "pd.DataFrame(confusion_matrix(twenty_test_small.target, predicted), \n",
      "             index = pd.MultiIndex.from_product([['actual'], twenty_test_small.target_names]),\n",
      "             columns = pd.MultiIndex.from_product([['predicted'], twenty_test_small.target_names]))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1-2 Pairs\n",
      "=========\n",
      "\n",
      "```bash\n",
      "$ unzip Classification_data -d Classification_data\n",
      "```\n",
      "\n",
      "1. Load the dateset using `load_files` (hint: our `categories` are now `spam`, `easy_ham`, etc.)\n",
      "2. Write a pre-processor callable to remove the message headers.\n",
      "3. Set up a pipeline for cross validation and model selection using `spam` and `easy_ham`.\n",
      "    - Which parameters should be optimized?\n",
      "    - Do you expect the results to be different from the parameters above? Why/why not?\n",
      "    - Are there other parameters we should optimize that we haven't tested?\n",
      "4. Use `GridSearchCV` to find optimal parameters for vectorizor and classifier.\n",
      "5. Run classifier against `hard_ham`. What percentage of `hard_ham` does it correctly identify as not `spam`?\n",
      "6. Display the most discriminative features. Anything stick out?\n",
      "7. Run classifier against `spam_2`, `easy_ham_2`, `hard_ham_2`. \n",
      "    - Plot the ROC curve (along with AUC) for each case. \n",
      "    - Print the confusion matrix"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Homework\n",
      "========\n",
      "\n",
      "1. Read the Na\u00efve Bayes documentation at [scikit-learn.org](http://scikit-learn.org/stable/modules/naive_bayes.html). There are three Na\u00efve Bayes classifiers described. Which of the other two might also be appropriate for this task?\n",
      "2. Explain your choice and apply it to either the spam/ham dataset (if you completed the pair assignment) or the newsgroups dataset (if you didn't).\n",
      "3. Use grid search cross validation to find the best parameters for both the vectorizor and classifier.\n",
      "    - Do different parameters for the vectorizor work better for this classifier?\n",
      "4. Does this classifier do better or worse than the multinomial classifier?\n",
      "5. Advanced: consider the descriptions of the two classifiers in light of which does better for this problem. Can you posit a theory as to why one classifier should do better than the other?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "",
  "signature": "sha256:1bb674ab217e1f8b211f8efb944835d3aff7bbd0f995d66ef09cc0cff610073e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab\n",
      "==========================================\n",
      "Big Data I: StarCluster & IPython.parallel\n",
      "------------------------------------------\n",
      "Alessandro D. Gagliardi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Wikipedia XML Data\n",
      "------------------------------------------\n",
      "A complete copy of all Wikimedia wikis, in the form of wikitext source and metadata embedded in XML.\n",
      "<table>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Size:\n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "500GB</td>\n",
      "          </tr>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Source:\n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "Wikimedia Foundation (http://download.wikipedia.org/backup-index.html)</td>\n",
      "          </tr>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Created On: \n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "<span class=\"timestamp\">May 15, 2009 12:09 AM GMT</span>\n",
      "\n",
      "            </td>\n",
      "          </tr>\n",
      "          <tr>\n",
      "            <td class=\"metaLeft\">\n",
      "              Last Updated:\n",
      "            </td>\n",
      "            <td class=\"metaRight\">\n",
      "              \n",
      "<span class=\"timestamp\">September 29, 2009  1:09 AM GMT</span>\n",
      "\n",
      "            </td>\n",
      "          </tr>\n",
      "        </table>  \n",
      "        \n",
      "http://aws.amazon.com/datasets/Encyclopedic/2506"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Part 1\n",
      "==========================================\n",
      "Preliminary investigation of our dataset\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import os\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "from boto.s3.connection import S3Connection\n",
      "from IPython.parallel import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "credentials = pd.read_csv('/Users/eklypse/Downloads/credentials.csv')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "s3conn = S3Connection(credentials['Access Key Id'][0], credentials['Secret Access Key'][0])\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wikipediaxml_keys = datasets.get_all_keys(prefix='wikipediaxml')\n",
      "len(wikipediaxml_keys)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[k.size for k in wikipediaxml_keys].index(min(k.size for k in wikipediaxml_keys))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# N.B. This takes a few minutes...\n",
      "data = wikipediaxml_keys[60].get_contents_as_string()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data[:500]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's explore the XML document object model using Python's minidom module:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from xml.dom import minidom\n",
      "dom = minidom.parseString(data.split('\\r')[0])\n",
      "print dom.firstChild.toprettyxml()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title = dom.firstChild.getElementsByTagName('title')[0]\n",
      "title.lastChild.wholeText"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "titles1 = [title.lastChild.wholeText \n",
      "     for entry in data.split('\\r')[:-1] \n",
      "         for title in minidom.parseString(entry).firstChild.getElementsByTagName('title')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Python XML module has another way of exploring XML as well:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from xml.etree import ElementTree\n",
      "ElementTree.fromstring(data.split('\\r')[0]).find('title').text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "titles2 = [ElementTree.fromstring(entry).find('title').text for entry in data.split('\\r')[:-1]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that both of these are extremely slow compared to simple string parsing:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import HTMLParser\n",
      "parser = HTMLParser.HTMLParser()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "titles3 = [parser.unescape(entry.split(u'</title>')[0]) for entry in data.split(u'<title>')[1:]]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles1 == titles2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles2 == titles3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(titles3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles3[::10000]"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Part 2\n",
      "==========================================\n",
      "Moving to StarCluster\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Add the `ipcluster` plugin if you haven't already."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Near the bottom of `.starcluster/config`:\n",
      "```bash\n",
      "######################\n",
      "## Built-in Plugins ##\n",
      "######################\n",
      "# The following plugins ship with StarCluster and should work out-of-the-box.\n",
      "# Uncomment as needed. Don't forget to update your PLUGINS list!\n",
      "# See http://star.mit.edu/cluster/docs/latest/plugins for plugin details.\n",
      "# .\n",
      "# .\n",
      "# .\n",
      "[plugin ipcluster]\n",
      "SETUP_CLASS = starcluster.plugins.ipcluster.IPCluster\n",
      "# Enable the IPython notebook server (optional)\n",
      "ENABLE_NOTEBOOK = True\n",
      "# Set a password for the notebook for increased security\n",
      "# This is optional but *highly* recommended\n",
      "NOTEBOOK_PASSWD = a-secret-password\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[tinyurl.com/EC2calc](https://docs.google.com/spreadsheet/ccc?key=0Asc8NbYa4sgCdGxQVnpQamtkUC1zYXRWcXF1R1lFcmc)\n",
      "\n",
      "In theory, 5 r3.4xlarge spot instances at 13\u00a2/hour should work but in practice, AWS has not let me reserve any r3.4xlarge (maybe you'll have better luck). \n",
      "\n",
      "Reserving 17 r3.xlarge instances didn't work for me either (despite the fact that AWS claims the default limit on that is 20) so I am stuck using the more expensive m3.2xlarge instances:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```bash\n",
      "###########################################\n",
      "## Defining Additional Cluster Templates ##\n",
      "###########################################\n",
      "# You can also define multiple cluster templates. You can either supply all\n",
      "# configuration options as with smallcluster above, or create an\n",
      "# EXTENDS=<cluster_name> variable in the new cluster section to use all\n",
      "# settings from <cluster_name> as defaults. Below are example templates that\n",
      "# use the EXTENDS feature:\n",
      "\n",
      "[cluster mediumcluster]\n",
      "# Declares that this cluster uses smallcluster as defaults\n",
      "EXTENDS=smallcluster\n",
      "# This section is the same as smallcluster except for the following settings:\n",
      "NODE_IMAGE_ID = ami-6b211202\n",
      "NODE_INSTANCE_TYPE = m3.2xlarge\n",
      "CLUSTER_SIZE = 17\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Start your new cluster:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`$ starcluster start -c mediumcluster my_cluster`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Copy your credentials to your cluster:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`$ starcluster put my_cluster --user sgeadmin ~/Downloads/credentials.csv /home/sgeadmin/`"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This should, as a side effect, add your cluster to the list of known hosts on your machine. If not, you will want to\n",
      "```bash\n",
      "starcluster sshmaster my_cluster\n",
      "```\n",
      "before you do the following (or `Client` will hang forever):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url_file = os.path.expanduser('~/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json')\n",
      "sshkey = '/Users/eklypse/.ssh/eklypse.pem'\n",
      "client = Client(url_file, sshkey = sshkey)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Check to see how many engines you have running:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview = client.direct_view()\n",
      "len(client.ids)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If there are fewer engines running than expected, that's probably because they didn't start on some of the instances. We can use the following to see which instances have engines:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#%%px\n",
      "#%%bash\n",
      "#ec2metadata --local-ipv4"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to set up each of our engines individually. We can do this quickly with the `%%px` magic:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px\n",
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "credentials = pd.read_csv('credentials.csv')\n",
      "\n",
      "s3conn = S3Connection(credentials['Access Key Id'][0], credentials['Secret Access Key'][0])\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We then scatter the keys to our S3 files across our cluster:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dview.scatter('wikipediaxml_keys', [key.name for key in wikipediaxml_keys], dist='r')"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If we were unable to investigate our data locally (perhaps the download failed) we can do so on engine zero by specifying the `target` thus:  \n",
      "*(only do this if the above attempt at running `get_contents_as_string` failed)* "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px --targets 0\n",
      "data = datasets.get_key(wikipediaxml_keys[0]).get_contents_as_string()\n",
      "data.split('\\r')[:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`-t0` is shorthand for `--targets 0`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px -t0\n",
      "titles = [entry.split('</title>')[0] for entry in data.split('<title>')[1:]]\n",
      "len(titles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px -t0\n",
      "titles[::10000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we run our analysis in parallel across 135 CPUs on 17 nodes. This will take a while, but consider the fact that this would be impossible your laptop:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px \n",
      "import pandas as pd\n",
      "from boto.s3.connection import S3Connection\n",
      "from collections import defaultdict\n",
      "\n",
      "credentials = pd.read_csv('credentials.csv')\n",
      "\n",
      "s3conn = S3Connection(credentials['Access Key Id'][0], credentials['Secret Access Key'][0])\n",
      "datasets = s3conn.get_bucket('datasets.elasticmapreduce')\n",
      "\n",
      "title_lens = defaultdict(int)\n",
      "errors = []\n",
      "\n",
      "for key_name in wikipediaxml_keys:\n",
      "    wikipediaxml_key = datasets.get_key(key_name)\n",
      "    try:\n",
      "        data = wikipediaxml_key.get_contents_as_string()\n",
      "        titles = [entry.split('</title>')[0] for entry in data.split('<title>')[1:]]\n",
      "        for title in titles:\n",
      "            title_tokens = title.split()\n",
      "            title_length = len(title_tokens)\n",
      "            title_lens[title_length] += 1\n",
      "    except:\n",
      "        errors.append(key_name)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "Intermission\n",
      "------------------------------------------\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_lens = dview.gather('title_lens').get()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series = pd.DataFrame(title_lens).sum(0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.plot()"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_len_series.tail()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "------------------------------------------\n",
      "# \u00a1\u00a1\u00a1DON'T FORGET TO TERMINATE YOUR CLUSTER BEFORE YOU LEAVE CLASS!!!\n",
      "------------------------------------------"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Homework\n",
      "==========================================\n",
      "```bash\n",
      "git pull https://github.com/ogrisel/parallel_ml_tutorial.git\n",
      "```\n",
      "Go through Olivier Grisel's [Parallel Machine Learning with scikit-learn and IPython](https://github.com/ogrisel/parallel_ml_tutorial) tutorials. Much of it will be a review. Pay attention to how `03 - Distributed Model Selection and Assessment` shows you how to do Grid Search over a StarCluster. But the *coup de gr\u00e2ce* is `05 - Large Scale Text Classification for Sentiment Analysis` which shows how to build a classification model using a large dataset in parallel across a cluster."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
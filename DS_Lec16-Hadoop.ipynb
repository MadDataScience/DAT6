{
 "metadata": {
  "celltoolbar": "Slideshow",
  "name": "",
  "signature": "sha256:9de314e7a72ef2df1ff6273bee9fa802079c89d1c17813850540e1d2e8efecfa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data Science\n",
      "================================\n",
      "Big Data II: Hadoop and EMR\n",
      "--------------------------------\n",
      "Alessandro D. Gagliardi"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Last Time:\n",
      "\n",
      "- #### Big Data\n",
      "- #### Amazon Web Services \n",
      "- #### StarCluster & IPython.parallel\n",
      "\n",
      "#### Questions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "### Agenda\n",
      "\n",
      "1. Lecture  \n",
      "    1. MapReduce Programming Model  \n",
      "    2. Implementation Details  \n",
      "    3. Word Count Example  \n",
      "2. Lab\n",
      "    1. Hadoop Streaming  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Recall: There are two ways to process data in a distributed architecture:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "1. move data to code (& processing power)\n",
      "    - SETI\n",
      "2. move code to data\n",
      "    - map-reduce -> less overhead (network traffic, disk I/O)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "\u201cComputing nodes are the same as storage nodes.\u201d"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "As we\u2019ve discussed, the map-reduce approach involves splitting a problem into subtasks and processing these subtasks in parallel."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This takes place in two phases:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "1. the **mapper** phase\n",
      "2. the **reducer** phase"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "As we\u2019ve discussed, the map-reduce approach involves splitting a problem into subtasks and processing these subtasks in parallel.\n",
      "\n",
      "This takes place in (*approximately*) two phases:\n",
      "\n",
      "1.  the **mapper** phase  \n",
      "2.  *shuffle/sort*    <-    the secret sauce  \n",
      "3.  the **reducer** phase"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "<CENTER><img src=http://mikecvet.files.wordpress.com/2010/07/mrfigure.png /></CENTER>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=https://developers.google.com/appengine/docs/python/images/mapreduce_mapshuffle.png>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Map-reduce uses a *functional programming* paradigm. The data processing *primitives* are mappers and reducers, as we\u2019ve seen."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "**mappers** \u2013 filter & transform data  \n",
      "**reducers** \u2013 aggregate results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = [1, 2, 3]\n",
      "b = [4, 5, 6, 7]\n",
      "c = [8, 9, 1, 2, 3]\n",
      "L = map(lambda x:len(x), [a, b, c])\n",
      "L"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "[3, 4, 5]"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N = reduce(lambda x, y: x+y, L)\n",
      "N"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "12"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Or, if we want to be fancy and do it in one line\n",
      "reduce(lambda x, y: x+y, map(lambda x:len(x), [a, b, c]))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "12"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "source": [
      "Thanks to [Michael Cvet](http://mikecvet.wordpress.com/2010/07/02/parallel-mapreduce-in-python/) for this example:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Map-reduce uses a functional programming paradigm. The data processing primitives are mappers and reducers, as we\u2019ve seen.  \n",
      "\n",
      "**mappers** \u2013 filter & transform data  \n",
      "**reducers** \u2013 aggregate results  \n",
      "\n",
      "The functional paradigm is good at describing how to solve a problem, but not very good at describing data manipulations (eg, relational joins)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "It\u2019s possible to overlay the map-reduce framework with an additional *declarative syntax*.\n",
      "\n",
      "This makes operations like select & join easier to implement and less error prone.\n",
      "\n",
      "Popular examples for Hadoop include Pig and Hive."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Why Pig?\n",
      "Because I bet you can read the following script."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "An example pig script:\n",
      "\n",
      "```\n",
      "users = load 'users.csv' as (username: chararray, age: int);\n",
      "users_1825 = filter users by age >= 18 and age <= 25;\n",
      "\n",
      "pages = load 'pages.csv' as (username: chararray, url: chararray);\n",
      "\n",
      "joined = join users_1825 by username, pages by username;\n",
      "grouped = group joined by url;\n",
      "summed = foreach grouped generate group as url, COUNT(joined) AS views;\n",
      "top_5 = limit sorted 5;\n",
      "\n",
      "store top_5 into 'top_5_sites.csv';\n",
      "```\n",
      "\n",
      "Now, just for fun... the same calculation in vanilla Hadoop MapReduce."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "<img src=\"assets/pig_02.png\">"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Implementation Details"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce is\u2026"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "A. A programming model for processing big data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "B. A data processing system created and used by Google"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "C. A feature of Hadoop"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "D. All of the above"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The implementation of MapReduce became popular when Google released a white paper in 2004 explaining how they did it. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This architecture was then copied. The two most popular versions are:\n",
      "\n",
      "- Apache\u2019s Hadoop\n",
      "- Disco (bet you haven\u2019t heard of this one)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "notes"
      }
     },
     "source": [
      "Disco more friendly but not as well supported. Hadoop has a huge community and so, while more difficult to use, is the standard."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "A map-reduce framework handles a lot of messy details for you:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "- parallelization & distribution (eg, input splitting)\n",
      "- partitioning (shuffle/sort/redirect)\n",
      "- fault-tolerance (fact: tasks/nodes will fail!)\n",
      "- I/O scheduling\n",
      "- status and monitoring"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This (along with the functional semantics) allows you to focus on solving the problem instead of accounting & housekeeping details."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "**Hadoop** is a popular open-source Java-based implementation of the map-reduce framework (including file storage for input/output)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "You can download Hadoop and configure a set of machines to operate as a map-reduce cluster, or you can run it as a *service* via Amazon\u2019s Elastic Map-Reduce."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "Hadoop is written in Java, but the *Hadoop Streaming* utility allows client code to be supplied as executables (eg, written in any language)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Data is replicated in the (distributed) file system across several nodes."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "This permits locality optimization (and fault tolerance) by allowing the mapper tasks to run on the same nodes where the data resides."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "So we move code to data (instead of data to code), thus avoiding a lot of network traffic and disk I/O."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"http://www.ndm.net/datawarehouse/images/stories/greenplum/GP-HD_HDFS-4-2.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The *Google File System* (GFS) was developed alongside map-reduce to serve as the native file system for this type of processing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "The Hadoop platform is bundled with an open-source implementation of this file system called *HDFS*."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "source": [
      "If you use Amazon EMR, you can use their file system (Amazon S3) as well. \n",
      " *(What's wrong with this?)*"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "C. Word Count Example"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "MapReduce processes data in terms of *key-value pairs*:\n",
      "\n",
      "    input           <k1, v1>\n",
      "\n",
      "    mapper          <k1, v1> -> <k2, v2>\n",
      "\n",
      "    (partitioner)   <k2, v2> -> <k2, [all k2 values]>\n",
      "\n",
      "    reducer         <k2, [all k2 values]> -> <k3, v3>\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "<img src=\"http://www.rabidgremlin.com/data20/MapReduceWordCountOverview1.png\">"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Using the following input, we can implement the \u201cHello World\u201d of map-reduce: a *word count*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "i = {1: 'where',\n",
      "     2: 'where in',\n",
      "     3: 'where in the',\n",
      "     4: 'where in the world',\n",
      "     5: 'where in the world is',\n",
      "     6: 'where in the world is carmen',\n",
      "     7: 'where in the world is carmen sandiego'}"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "First, as for comparison, this is one way you could do it on a single machine:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "wordcount = defaultdict(int)\n",
      "\n",
      "for _, s in i.items():\n",
      "    for w in s.split():\n",
      "        wordcount[w] += 1\n",
      "\n",
      "reducer_results = pd.Series(wordcount)\n",
      "reducer_results.sort()\n",
      "reducer_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "sandiego    1\n",
        "carmen      2\n",
        "is          3\n",
        "world       4\n",
        "the         5\n",
        "in          6\n",
        "where       7\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The first processing primitive is the mapper, which filters & transforms the input data, and *yields* transformed key-value pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def mapper(k1, v1):\n",
      "    '''k1: line number\n",
      "       v1: line contents (i.e. space-delimited string)'''\n",
      "    words = v1.split()  # split string into words\n",
      "    for word in words:\n",
      "        yield (word, 1)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The mapper emits key-value pairs for each word encountered in the input data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import chain\n",
      "map_results = list(chain.from_iterable(map(mapper, i.keys(), i.values())))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "map_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[('where', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('carmen', 1),\n",
        " ('where', 1),\n",
        " ('in', 1),\n",
        " ('the', 1),\n",
        " ('world', 1),\n",
        " ('is', 1),\n",
        " ('carmen', 1),\n",
        " ('sandiego', 1)]"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "The partitioner is internal to the map-reduce framework, so we don\u2019t have to write this ourselves. It shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a *single* reducer."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "subslide"
      }
     },
     "source": [
      "If we were using a map-reduce framework like Hadoop or Disco, the partitioner would be internal. But since we are building this from scratch, we will create our own partitioner using a `defaultdict`. The partitioner shuffles & sorts the mapper output, and redirects all intermediate results for a given key to a *single* reducer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "partition_results = defaultdict(list)\n",
      "for k, v in map_results:\n",
      "    partition_results[k].append(v)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pd.Series(partition_results)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "carmen                     [1, 1]\n",
        "in             [1, 1, 1, 1, 1, 1]\n",
        "is                      [1, 1, 1]\n",
        "sandiego                      [1]\n",
        "the               [1, 1, 1, 1, 1]\n",
        "where       [1, 1, 1, 1, 1, 1, 1]\n",
        "world                [1, 1, 1, 1]\n",
        "dtype: object"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Finally, the reducer receives all values for a given key and aggregates (in this case, sums) the results"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reducer(k2, k2_vals):\n",
      "    '''k2: word\n",
      "       k2_vals: word counts'''\n",
      "    yield k2, sum(k2_vals)"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "Reducer output is aggregated & sorted by key."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results = dict(chain.from_iterable(map(reducer, partition_results.keys(), partition_results.values())))"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "skip"
      }
     },
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reducer_results"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "fragment"
      }
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "{'carmen': 2,\n",
        " 'in': 6,\n",
        " 'is': 3,\n",
        " 'sandiego': 1,\n",
        " 'the': 5,\n",
        " 'where': 7,\n",
        " 'world': 4}"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "# Questions?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "LAB\n",
      "===\n",
      "In the `DAT6` folder, from the command line:\n",
      "```bash\n",
      "git commit -am ...\n",
      "git checkout gh-pages\n",
      "git pull\n",
      "git checkout personal\n",
      "git merge gh-pages\n",
      "ipython notebook\n",
      "```\n",
      "Then open `DS_Lab16-Hadoop`"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
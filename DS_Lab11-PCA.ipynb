{
 "metadata": {
  "name": "",
  "signature": "sha256:0acb55bb6ddfc59ae0f812e1583777c43ed1f55dca26cafdee2f2ba5d737068f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Lab\n",
      "==========================================\n",
      "Unsupervised Learning: Dimensionality Reduction and Visualization\n",
      "------------------------------------------\n",
      "Alessandro D. Gagliardi  \n",
      "*(adapted from Jake VanderPlas' tutorial)*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "from itertools import cycle\n",
      "from sklearn.datasets import load_files\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# Some nice default configuration for plots\n",
      "plt.rcParams['figure.figsize'] = 10, 7.5\n",
      "plt.rcParams['axes.grid'] = True\n",
      "plt.gray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unsupervised learning is interested in situations in which X is available, but not y: data without labels.\n",
      "\n",
      "A typical use case is to find hiden structure in the data.\n",
      "\n",
      "Previously we worked on visualizing the iris data by plotting\n",
      "pairs of dimensions by trial and error, until we arrived at\n",
      "the best pair of dimensions for our dataset.  Here we will\n",
      "use an unsupervised *dimensionality reduction* algorithm\n",
      "to accomplish this more automatically."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By the end of this section you will\n",
      "\n",
      "- Know how to instantiate and train an unsupervised dimensionality reduction algorithm:\n",
      "  Principal Component Analysis (PCA)\n",
      "- Know how to use PCA to visualize high-dimensional data"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Dimensionality Reduction: PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dimensionality reduction is the task of deriving a set of new\n",
      "artificial features that is smaller than the original feature\n",
      "set while retaining most of the variance of the original data.\n",
      "Here we'll use a common but powerful dimensionality reduction\n",
      "technique called Principal Component Analysis (PCA).\n",
      "We'll perform PCA on the iris dataset that we saw before:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_iris\n",
      "iris = load_iris()\n",
      "X = iris.data\n",
      "y = iris.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA is performed using linear combinations of the original features\n",
      "using a truncated Singular Value Decomposition of the matrix X so\n",
      "as to project the data onto a base of the top singular vectors.\n",
      "If the number of retained components is 2 or 3, PCA can be used\n",
      "to visualize the dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.decomposition import PCA\n",
      "pca = PCA(n_components=2, whiten=True)\n",
      "pca.fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once fitted, the pca model exposes the singular vectors in the components_ attribute:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.components_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Other attributes are available as well:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca.explained_variance_ratio_.sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us project the iris dataset along those first two dimensions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_pca = pca.transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA `normalizes` and `whitens` the data, which means that the data\n",
      "is now centered on both components with unit variance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.round(X_pca.mean(axis=0), decimals=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.round(X_pca.std(axis=0), decimals=5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Furthermore, the samples components do no longer carry any linear correlation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.corrcoef(X_pca.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can visualize the projection using pylab, but first\n",
      "let's make sure our ipython notebook is in pylab inline mode"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can visualize the results using the following utility function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import matplotlib.pyplot as plt\n",
      "from itertools import cycle\n",
      "\n",
      "def plot_PCA_2D(data, target, target_names):\n",
      "    colors = cycle('rgbcmykw')\n",
      "    target_ids = range(len(target_names))\n",
      "    plt.figure()\n",
      "    for i, c, label in zip(target_ids, colors, target_names):\n",
      "        plt.scatter(data[target == i, 0], data[target == i, 1],\n",
      "                   c=c, label=label)\n",
      "    plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now calling this function for our data, we see the plot:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_PCA_2D(X_pca, iris.target, iris.target_names)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that this projection was determined *without* any information about the\n",
      "labels (represented by the colors): this is the sense in which the learning\n",
      "is **unsupervised**.  Nevertheless, we see that the projection gives us insight\n",
      "into the distribution of the different flowers in parameter space: notably,\n",
      "*iris setosa* is much more distinct than the other two species."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note also that the default implementation of PCA computes the\n",
      "singular value decomposition (SVD) of the full\n",
      "data matrix, which is not scalable when both ``n_samples`` and\n",
      "``n_features`` are big (more that a few thousands).\n",
      "If you are interested in a number of components that is much\n",
      "smaller than both ``n_samples`` and ``n_features``, consider using\n",
      "`sklearn.decomposition.RandomizedPCA` instead."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Other dimensionality reduction techniques which are useful to know about:\n",
      "\n",
      "- [sklearn.decomposition.PCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.PCA.html): \n",
      "   Principal Component Analysis\n",
      "- [sklearn.decomposition.RandomizedPCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.RandomizedPCA.html):\n",
      "   fast non-exact PCA implementation based on a randomized algorithm\n",
      "- [sklearn.decomposition.SparsePCA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.SparsePCA.html):\n",
      "   PCA variant including L1 penalty for sparsity\n",
      "- [sklearn.decomposition.FastICA](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.FastICA.html):\n",
      "   Independent Component Analysis\n",
      "- [sklearn.decomposition.NMF](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.NMF.html):\n",
      "   non-negative matrix factorization\n",
      "- [sklearn.manifold.LocallyLinearEmbedding](http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html):\n",
      "   nonlinear manifold learning technique based on local neighborhood geometry\n",
      "- [sklearn.manifold.IsoMap](http://scikit-learn.org/0.13/modules/generated/sklearn.manifold.Isomap.html):\n",
      "   nonlinear manifold learning technique based on a sparse graph algorithm\n",
      "- [sklearn.decomposition.TruncatedSVD](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.TruncatedSVD.html): \n",
      "   linear dimensionality reduction by means of truncated singular value decomposition (SVD)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1-2 Pairs\n",
      "=========\n",
      "1. Randomized PCA\n",
      "--------------------------------\n",
      "#### Repeat the above dimensionality reduction with ``sklearn.decomposition.RandomizedPCA``.  \n",
      "You can re-use the ``plot_PCA_2D`` function from above.  \n",
      "Are the results similar to those from standard PCA?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2. Dimension Reduction of Digits"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Apply PCA (or RandomizedPCA?) to the Hand Written Digits dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import load_digits\n",
      "digits = load_digits()\n",
      "print(digits.DESCR)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X, y = digits.data, digits.target\n",
      "\n",
      "print(\"data shape: %r, target shape: %r\" % (X.shape, y.shape))\n",
      "print(\"classes: %r\" % list(np.unique(y)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_samples, n_features = X.shape\n",
      "print(\"n_samples=%d\" % n_samples)\n",
      "print(\"n_features=%d\" % n_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_gallery(data, labels, shape, interpolation='nearest'):\n",
      "    for i in range(data.shape[0]):\n",
      "        plt.subplot(1, data.shape[0], (i + 1))\n",
      "        plt.imshow(data[i].reshape(shape), interpolation=interpolation)\n",
      "        plt.title(labels[i])\n",
      "        plt.xticks(()), plt.yticks(())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subsample = np.random.permutation(X.shape[0])[:5]\n",
      "images = X[subsample]\n",
      "labels = ['True class: %d' % l for l in y[subsample]]\n",
      "plot_gallery(images, labels, shape=(8, 8))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Homework\n",
      "=========\n",
      "Recall the newsgroups dataset we used in [DS_Lab09-TM](DS_Lab09-TM.ipynb). Once we have extracted a vector representation of the data, it's a good idea to project the data on the first 2D of a Singular Value Decomposition (i.e.. Principal Component Analysis) to get a feel of the data. Note that the [TruncatedSVD](http://scikit-learn.org/0.13/modules/generated/sklearn.decomposition.TruncatedSVD.html) class can accept `scipy.sparse` matrices as input (as an alternative to numpy arrays). Use it to visualize the first two principal components of the vectorized dataset. Color the output with the newsgroup names as we did with the iris data. Do you notice anything interesting about where `alt.atheism` and `talk.religion.misc` cluster relative to `sci.space`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
      "twenty_train_small = load_files('datasets/20news-bydate-train/', categories=categories, encoding='latin-1')\n",
      "y_train = twenty_train_small.target"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "vectorizer = TfidfVectorizer(min_df=1)\n",
      "\n",
      "X_train_small = vectorizer.fit_transform(twenty_train_small.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}